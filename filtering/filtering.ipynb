{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc860e4f-4f97-46d6-9110-f6e8c498abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from trainer import Qwen2VLForConditionalGeneration_SelfFilter\n",
    "from transformers.modeling_utils import load_sharded_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad69c09-dc88-4e8c-add5-93221b21bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stage1_model(\n",
    "    model_path, feature_extractor_setting='clip', device_map=\"auto\", device=\"cuda\", **kwargs\n",
    "):\n",
    "    kwargs = {\"device_map\": device_map, **kwargs}\n",
    "\n",
    "    if device != \"cuda\":\n",
    "        kwargs[\"device_map\"] = {\"\": device}\n",
    "\n",
    "    kwargs[\"torch_dtype\"] = torch.float16\n",
    "\n",
    "    # note that we do not need vision tower here, and it is not loaded.\n",
    "    if feature_extractor_setting == \"clip\":\n",
    "        model = Qwen2VLForConditionalGeneration_SelfFilter.from_pretrained(\n",
    "            model_path, low_cpu_mem_usage=True, **kwargs\n",
    "        ).to(device)\n",
    "    elif feature_extractor_setting == \"scores\":\n",
    "        model = LlavaLlamaForCausalLM_SelfFilter_Scores.from_pretrained(\n",
    "            model_path, ignore_mismatched_sizes=True, low_cpu_mem_usage=True, **kwargs\n",
    "        ).to(device)\n",
    "    else:\n",
    "        print(\"Unknown feature extractor setting: \", feature_extractor_setting)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    non_lora_state_path = os.path.join(model_path, \"non_lora_trainables.bin\")\n",
    "    if os.path.exists(non_lora_state_path):\n",
    "        non_lora_state_dict = torch.load(non_lora_state_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(non_lora_state_dict, strict=False)\n",
    "        print('score_net has been well loaded.')\n",
    "    else:\n",
    "        print(\"Warning: non_lora_trainables.bin 파일이 없습니다.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572e96cf-5554-4cfe-b6a9-7503225fee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(score_names):\n",
    "    def norm_scores(score_dict: dict):\n",
    "        min_score = min(score_dict.values())\n",
    "        max_score = max(score_dict.values())\n",
    "        normed_score_dict = {\n",
    "            i[0]: (i[1] - min_score) / (max_score - min_score) * 2 - 1\n",
    "            for i in score_dict.items()\n",
    "        }\n",
    "        return normed_score_dict\n",
    "\n",
    "    score_dicts = []\n",
    "\n",
    "    for score_name in score_names:\n",
    "        with open(score_name, \"r\") as f:\n",
    "            score_dict = json.load(f)\n",
    "            score_dicts.append(norm_scores(score_dict))\n",
    "\n",
    "    return score_dicts\n",
    "\n",
    "def produce_scores_difficulty(model, save_path: str):\n",
    "    difficulty_dict = {}\n",
    "\n",
    "    score_files = [\n",
    "        \"llava_imagereward.json\",\n",
    "        \"llava_clipscore.json\",\n",
    "        #\"data/scores/gpt-3.5-turbo-1106/processed_score.json\",\n",
    "    ]\n",
    "    score_dicts = [json.load(open(file, \"r\")) for file in score_files]\n",
    "\n",
    "    for unique_idx in score_dicts[0]:\n",
    "        scores = [[score_dict[str(unique_idx)] for score_dict in score_dicts]]\n",
    "        scores = torch.tensor(scores).cuda().half()\n",
    "        difficulty_dict[unique_idx] = -model.predict_weights(scores).item()\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(difficulty_dict, f)\n",
    "\n",
    "    print(\"Scores difficulty generated and saved.\")\n",
    "\n",
    "    return difficulty_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f053176-787c-4029-990c-27b61d53efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_clip_difficulty(model, save_path: str):\n",
    "    difficulty_dict = {}\n",
    "    clip_feat = torch.load(\"/workspace/Self-Filter/llava_clip_feature.pt\")\n",
    "\n",
    "    for unique_idx in clip_feat:\n",
    "        dtype = model.get_score_net_dtype()\n",
    "        scores = clip_feat[unique_idx].unsqueeze(dim=0).cuda().to(dtype=dtype)\n",
    "        difficulty_dict[unique_idx] = -model.predict_weights(scores).item()\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(difficulty_dict, f)\n",
    "\n",
    "    print(\"CLIP difficulty generated and saved\")\n",
    "\n",
    "    return difficulty_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6305ac6-4113-44d6-9c89-d1b2740515b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difficulty_score(\n",
    "    model_path: str, feature_extractor_setting: str, save_path: str\n",
    "):\n",
    "    print(\"Loading stage 1 model...\", flush=True)\n",
    "    model = load_stage1_model(model_path, feature_extractor_setting)\n",
    "    print(\"Model loaded.\", flush=True)\n",
    "\n",
    "    if feature_extractor_setting == \"scores\":\n",
    "        return produce_scores_difficulty(model, save_path)\n",
    "    else:\n",
    "        return produce_clip_difficulty(model, save_path)\n",
    "\n",
    "\n",
    "def dist_filter(\n",
    "    raw_annotation_path, difficulty_dict, filter_num, save_path, gamma=1, k_nearest=10\n",
    "):\n",
    "\n",
    "    with open(raw_annotation_path, \"r\") as f:\n",
    "        raw_annotation = json.load(f)\n",
    "    new_annotation = []\n",
    "\n",
    "    feat_dict = torch.load(\"../data/llava_clip_feature.pt\")\n",
    "    feat_len = len(feat_dict)\n",
    "    feat_matrix = torch.stack(\n",
    "        [feat_dict[str(i)].cuda() for i in range(feat_len)], dim=0\n",
    "    )\n",
    "    feat_matrix_norm = torch.norm(feat_matrix, dim=-1, keepdim=False)\n",
    "\n",
    "    for i in tqdm.tqdm(range(filter_num)):\n",
    "        lst = sorted(difficulty_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        unique_idx, difficulty = lst[0]\n",
    "\n",
    "        example = raw_annotation[int(unique_idx)]\n",
    " \n",
    "\n",
    "        example.pop(\"unique_idx\")\n",
    "        new_annotation.append(example)\n",
    "\n",
    "        difficulty_dict.pop(unique_idx)\n",
    "\n",
    "        tgt_feat = feat_matrix[int(unique_idx)].unsqueeze(dim=0)\n",
    "        tgt_norm = feat_matrix_norm[int(unique_idx)].unsqueeze(dim=0)\n",
    "\n",
    "        sims = (feat_matrix * tgt_feat).sum(dim=-1) / feat_matrix_norm / tgt_norm\n",
    "\n",
    "        sorted_sim, indices = torch.sort(sims, descending=True)\n",
    "\n",
    "        success_cnt = 0\n",
    "\n",
    "        for j in range(len(difficulty_dict)):\n",
    "            if success_cnt >= k_nearest:\n",
    "                break\n",
    "\n",
    "            cur_unique_idx = str(indices[j].item())\n",
    "\n",
    "            if cur_unique_idx not in difficulty_dict:\n",
    "                continue\n",
    "\n",
    "            cur_sim = sorted_sim[j].item()\n",
    "            penalty = difficulty * (cur_sim**2) * gamma\n",
    "            difficulty_dict[cur_unique_idx] -= penalty\n",
    "            success_cnt += 1\n",
    "\n",
    "        assert success_cnt == k_nearest\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(new_annotation, f)\n",
    "\n",
    "    print(\"Annotation filtered and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "746689dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stage 1 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cc4329c2f846c5a7de57e9af04abf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2VLForConditionalGeneration_SelfFilter were not initialized from the model checkpoint at /workspace/Self-Filter/checkpoint/Qwen2-VL-7B-Instruct and are newly initialized: ['score_net.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_net has been well loaded.\n",
      "Model loaded.\n",
      "CLIP difficulty generated and saved\n"
     ]
    }
   ],
   "source": [
    "stage1_model_path = \"../checkpoint/qwen2vl_clip_lora\"\n",
    "\n",
    "difficulty_dict = get_difficulty_score(\n",
    "    stage1_model_path,\n",
    "    'clip',\n",
    "    \"../data/difficulty_clip_qwenvl2_lora.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad83c14-9c56-4614-858a-3f30657a5dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [08:12<00:00, 60.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation filtered and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist_filter(\n",
    "    raw_annotation_path = '../data/llava_instruct_80k_add_idx.json',\n",
    "    difficulty_dict = difficulty,\n",
    "    filter_num = 30000,\n",
    "    save_path = \"../data/qwen2vl_filtered_lora_30k.json\",\n",
    "    gamma = 1,\n",
    "    k_nearest = 10,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
